{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192a96b6-f66d-4024-bbe1-2b957b2e0553",
   "metadata": {},
   "source": [
    "Now we have the pieces in place to acutally train a network to segment images for us. Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ecba77-25b7-41e4-9fcc-fa08dde89319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data import DSBData, get_dsb2018_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7a47269-55cf-4e85-9aa0-fc614a44c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai.networks.nets import BasicUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832bcf74-942e-43d5-812d-d22234562d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 382/382 [00:20<00:00, 18.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n"
     ]
    }
   ],
   "source": [
    "train_img_files, train_lbl_files = get_dsb2018_train_files()\n",
    "\n",
    "train_data = DSBData(\n",
    "    image_files=train_img_files,\n",
    "    label_files=train_lbl_files,\n",
    "    target_shape=(256, 256)\n",
    ")\n",
    "\n",
    "print(len(train_data))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f192cad-4454-4e16-9f3c-5d589990ce25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicUNet features: (16, 16, 32, 64, 128, 16).\n"
     ]
    }
   ],
   "source": [
    "model = BasicUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    features=[16, 16, 32, 64, 128, 16],\n",
    "    act=\"relu\",\n",
    "    norm=\"batch\",\n",
    "    dropout=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2fcf43-e3a9-4baf-a337-028274cbf4e2",
   "metadata": {},
   "source": [
    "Training of a neural network means updating its parameters (weights) using a strategy that involves the gradients of a loss function with respect to the model parameters in order to adjust model weights to minimize this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4252176f-df86-41e9-a987-3fee16166d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.e-3)\n",
    "init_params = list(model.parameters())[0].clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8c1d3-b285-4b7e-b2b3-91093ad1bfd3",
   "metadata": {},
   "source": [
    "Such a training is performed by iterating over the batches of the training dataset multiple times. Each full iteration over the dataset is termed an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f204265-f5e9-49c8-9d85-3a4721008db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Batch: 0 Total samples processed 32 Loss: 0.7636604309082031\n",
      "Train Epoch: 1 Batch: 1 Total samples processed 64 Loss: 0.7316972017288208\n",
      "Train Epoch: 1 Batch: 2 Total samples processed 96 Loss: 0.6992580890655518\n",
      "Train Epoch: 1 Batch: 3 Total samples processed 128 Loss: 0.6787922382354736\n",
      "Train Epoch: 1 Batch: 4 Total samples processed 160 Loss: 0.6548287868499756\n",
      "Train Epoch: 1 Batch: 5 Total samples processed 192 Loss: 0.6310180425643921\n",
      "Train Epoch: 1 Batch: 6 Total samples processed 224 Loss: 0.6482836604118347\n",
      "Train Epoch: 1 Batch: 7 Total samples processed 256 Loss: 0.6311068534851074\n"
     ]
    }
   ],
   "source": [
    "max_nepochs = 1\n",
    "log_interval = 1\n",
    "model.train(True)\n",
    "\n",
    "# expects raw unnormalized scores and combines sigmoid + BCELoss for better\n",
    "# numerical stability.\n",
    "# expects B x C x W x D\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "for epoch in range(1, max_nepochs + 1):\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # print(\"train\", batch_idx, X.shape, y.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction_logits = model(X)\n",
    "        # convert to 0/1 range on each pixel\n",
    "        # prediction = torch.nn.functional.sigmoid(prediction_logits)\n",
    "\n",
    "        batch_loss = loss_function(prediction_logits, y)\n",
    "\n",
    "        batch_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch:\",\n",
    "                epoch,\n",
    "                \"Batch:\",\n",
    "                batch_idx,\n",
    "                \"Total samples processed\",\n",
    "                (batch_idx + 1) * train_loader.batch_size,\n",
    "                \"Loss:\",\n",
    "                batch_loss.item(),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a714ad71-ea30-484f-be8d-2a4518b0937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params = list(model.parameters())[0].clone().detach()\n",
    "assert not torch.allclose(init_params, final_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
